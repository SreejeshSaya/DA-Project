{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import  date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Import DecisionTreeClassifier from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "plt.style.use('ggplot')\n",
    "df=pd.read_csv('US_accidents_for_5_states.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Augmentation - Extracting time of Accident and splitting into different fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Start_Time'] = pd.to_datetime(df['Start_Time'], errors='coerce')\n",
    "df['End_Time'] = pd.to_datetime(df['End_Time'], errors='coerce')\n",
    "\n",
    "# Extract year, month, day, hour and weekday\n",
    "df['Year']=df['Start_Time'].dt.year\n",
    "df['Month']=df['Start_Time'].dt.strftime('%b')\n",
    "df['Day']=df['Start_Time'].dt.day\n",
    "df['Hour']=df['Start_Time'].dt.hour\n",
    "df['Weekday']=df['Start_Time'].dt.strftime('%a')\n",
    "\n",
    "# Extract the amount of time in the unit of minutes for each accident, round to the nearest integer\n",
    "td='Time_Duration(min)'\n",
    "df[td]=round((df['End_Time']-df['Start_Time'])/np.timedelta64(1,'m'))\n",
    "df.info()\n",
    "\n",
    "\n",
    "dd=df.copy()\n",
    "\n",
    "\n",
    "dd=dd[dd['State']=='KY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the columns that have a more than half NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dd))\n",
    "\n",
    "print(len(dd.columns))\n",
    "cols = dd.columns[dd.isnull().mean()>0.5]\n",
    "dd.drop(cols, axis=1,inplace=True)\n",
    "\n",
    "print(dd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The list unwantedCols list contains the columns that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted_cols=['Turning_Loop','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight','Weather_Timestamp','TMC']\n",
    "dd.drop(unwanted_cols, axis=1,inplace=True)\n",
    "\n",
    "print(dd.shape)\n",
    "print(dd.isnull().sum())\n",
    "print(len(dd))\n",
    "\n",
    "\n",
    "import math\n",
    "bin_size=math.floor(1+3.322*math.log(len(dd),10))\n",
    "print(bin_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the box plots and finding the outliers\n",
    "sns.boxplot(data=dd,x=dd['Temperature(F)'])\n",
    "Q1=dd['Temperature(F)'].quantile(0.25)\n",
    "Q3=dd['Temperature(F)'].quantile(0.75)\n",
    "IQR=Q3-Q1\n",
    "print(Q1)\n",
    "print(Q3)\n",
    "print(IQR)\n",
    "iqr_range=1.5*IQR\n",
    "Lower_bound=Q1-iqr_range\n",
    "Upper_bound=Q3+iqr_range\n",
    "lower_outliers=dd[dd['Temperature(F)']<Lower_bound]\n",
    "upper_outliers=dd[dd['Temperature(F)']>Upper_bound]\n",
    "print(len(lower_outliers),len(upper_outliers))\n",
    "print(dd['Temperature(F)'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling the temperature column missing values with its mean\n",
    "dd_temperature=dd[\"Temperature(F)\"]\n",
    "#plotting the temperature histogram\n",
    "dd_temperature.hist(bins=bin_size)\n",
    "#approximately follows normal distribution\n",
    "print(dd[\"Temperature(F)\"].isnull().sum())\n",
    "#filling the missing values with the median\n",
    "median_temperature=dd_temperature.median()\n",
    "mean_temperature=dd_temperature.mean()\n",
    "print(mean_temperature,median_temperature)\n",
    "print(median_temperature)\n",
    "#filling the median temperature into the missing values since we are keeping the outliers\n",
    "dd[\"Temperature(F)\"].fillna(median_temperature, inplace=True)\n",
    "#ensuring that there are no more null values in temperature\n",
    "print(dd['Temperature(F)'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the box plots and finding the outliers\n",
    "sns.boxplot(data=dd,x=dd['Humidity(%)'])\n",
    "Q1=dd['Humidity(%)'].quantile(0.25)\n",
    "Q3=dd['Humidity(%)'].quantile(0.75)\n",
    "IQR=Q3-Q1\n",
    "print(Q1)\n",
    "print(Q3)\n",
    "print(IQR)\n",
    "iqr_range=1.5*IQR\n",
    "Lower_bound=Q1-iqr_range\n",
    "Upper_bound=Q3+iqr_range\n",
    "lower_outliers=dd[dd['Humidity(%)']<Lower_bound]\n",
    "upper_outliers=dd[dd['Humidity(%)']>Upper_bound]\n",
    "print(len(lower_outliers),len(upper_outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling the humidity column missing values with its mean\n",
    "dd_humidity=dd[\"Humidity(%)\"]\n",
    "#plotting the humidity histogram\n",
    "dd_humidity.hist(bins=bin_size)\n",
    "#approximately follows normal distribution\n",
    "print(dd[\"Humidity(%)\"].isnull().sum())\n",
    "#filling the missing values with the mean\n",
    "median_humidity=dd_humidity.median()\n",
    "mean_humidity=dd_humidity.mean()\n",
    "print(mean_humidity,median_humidity)\n",
    "#calculating the median and mean humidity and it is left-skewed ditribution\n",
    "print(mean_humidity)\n",
    "#filling the median temperature into the missing values since we are keeping the outliers\n",
    "dd[\"Humidity(%)\"].fillna(mean_humidity, inplace=True)\n",
    "#ensuring that there are no more null values in temperature\n",
    "print(dd['Humidity(%)'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the box plots and finding the outliers\n",
    "sns.boxplot(data=dd,x=dd['Pressure(in)'])\n",
    "Q1=dd['Pressure(in)'].quantile(0.25)\n",
    "Q3=dd['Pressure(in)'].quantile(0.75)\n",
    "IQR=Q3-Q1\n",
    "print(Q1)\n",
    "print(Q3)\n",
    "print(IQR)\n",
    "iqr_range=1.5*IQR\n",
    "Lower_bound=Q1-iqr_range\n",
    "Upper_bound=Q3+iqr_range\n",
    "lower_outliers=dd[dd['Pressure(in)']<Lower_bound]\n",
    "upper_outliers=dd[dd['Pressure(in)']>Upper_bound]\n",
    "print(len(lower_outliers),len(upper_outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the concept of flooring and capping\n",
    "quartile_10=dd['Pressure(in)'].quantile(0.10)\n",
    "print(quartile_10) #29.28\n",
    "quartile_90=dd['Pressure(in)'].quantile(0.90)\n",
    "print(quartile_90)\n",
    "print(dd['Pressure(in)'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling the Pressure column missing values with its mean\n",
    "dd_Pressure=dd[\"Pressure(in)\"]\n",
    "#plotting the Pressure histogram\n",
    "dd_Pressure.hist(bins=bin_size)\n",
    "#approximately follows normal distribution\n",
    "print(dd[\"Pressure(in)\"].isnull().sum())\n",
    "#filling the missing values with the concept of flooring and capping\n",
    "median_Pressure=dd_Pressure.median()\n",
    "mean_Pressure=dd_Pressure.mean()\n",
    "print(mean_Pressure,median_Pressure)\n",
    "dd[\"Pressure(in)\"] = np.where(dd[\"Pressure(in)\"] <quartile_10, quartile_10,dd['Pressure(in)'])\n",
    "dd[\"Pressure(in)\"] = np.where(dd[\"Pressure(in)\"] >quartile_90, quartile_90,dd['Pressure(in)'])\n",
    "print(dd['Pressure(in)'].skew())\n",
    "new_median_Pressure=dd_Pressure.median()\n",
    "new_mean_Pressure=dd_Pressure.mean()\n",
    "print(new_mean_Pressure,new_median_Pressure)\n",
    "dd[\"Pressure(in)\"].fillna(new_median_Pressure, inplace=True)\n",
    "#ensuring that there are no more null values in pressure\n",
    "print(dd['Pressure(in)'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visibility\n",
    "\n",
    "#plotting the box plots and finding the outliers\n",
    "sns.boxplot(data=dd,x=dd['Visibility(mi)'])\n",
    "Q1=dd['Visibility(mi)'].quantile(0.25)\n",
    "Q3=dd['Visibility(mi)'].quantile(0.75)\n",
    "IQR=Q3-Q1\n",
    "print(Q1)\n",
    "print(Q3)\n",
    "print(IQR)\n",
    "iqr_range=1.5*IQR\n",
    "Lower_bound=Q1-iqr_range\n",
    "Upper_bound=Q3+iqr_range\n",
    "lower_outliers=dd[dd['Visibility(mi)']<Lower_bound]\n",
    "upper_outliers=dd[dd['Visibility(mi)']>Upper_bound]\n",
    "print(len(lower_outliers),len(upper_outliers))\n",
    "\n",
    "\n",
    "#it is skewed\n",
    "\n",
    "\n",
    "#using the concept of flooring and capping\n",
    "quartile_10=dd['Visibility(mi)'].quantile(0.10) \n",
    "quartile_90=dd['Visibility(mi)'].quantile(0.90) \n",
    "print(dd['Visibility(mi)'].skew())  #4.37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling the Visibility column missing values with its mean\n",
    "dd_Visibility=dd[\"Visibility(mi)\"]\n",
    "#plotting the Pressure histogram\n",
    "dd_Visibility.hist(bins=bin_size)\n",
    "#approximately follows normal distribution\n",
    "print(dd[\"Visibility(mi)\"].isnull().sum())\n",
    "#filling the missing values with the concept of flooring and capping\n",
    "median_Visibility=dd_Visibility.median()\n",
    "mean_Visibility=dd_Visibility.mean()\n",
    "print(mean_Visibility,median_Visibility)\n",
    "#calculating the median and mean pressure and it is left-skewed ditribution\n",
    "dd[\"Visibility(mi)\"] = np.where(dd[\"Visibility(mi)\"] <quartile_10, quartile_10,dd['Visibility(mi)'])\n",
    "dd[\"Visibility(mi)\"] = np.where(dd[\"Visibility(mi)\"] >quartile_90, quartile_90,dd['Visibility(mi)'])\n",
    "print(dd['Visibility(mi)'].skew())\n",
    "#new mean and median\n",
    "new_median_Visibility=dd_Visibility.median()\n",
    "new_mean_Visibility=dd_Visibility.mean()\n",
    "print(new_mean_Visibility,new_median_Visibility)\n",
    "dd[\"Visibility(mi)\"].fillna(new_median_Visibility, inplace=True)\n",
    "#ensuring that there are no more null values in visibility\n",
    "print(dd['Visibility(mi)'].isnull().sum())\n",
    "\n",
    "print(dd.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the box plots and finding the outliers\n",
    "sns.boxplot(data=dd,x=dd['Wind_Speed(mph)'])\n",
    "Q1=dd['Wind_Speed(mph)'].quantile(0.25)\n",
    "Q3=dd['Wind_Speed(mph)'].quantile(0.75)\n",
    "IQR=Q3-Q1\n",
    "print(Q1)\n",
    "print(Q3)\n",
    "print(IQR)\n",
    "iqr_range=1.5*IQR\n",
    "Lower_bound=Q1-iqr_range\n",
    "Upper_bound=Q3+iqr_range\n",
    "lower_outliers=dd[dd['Wind_Speed(mph)']<Lower_bound]\n",
    "upper_outliers=dd[dd['Wind_Speed(mph)']>Upper_bound]\n",
    "print(len(lower_outliers),len(upper_outliers))\n",
    "\n",
    "\n",
    "#it is skewed\n",
    "\n",
    "\n",
    "#using the concept of flooring and capping\n",
    "quartile_10=dd['Wind_Speed(mph)'].quantile(0.10) #0\n",
    "quartile_90=dd['Wind_Speed(mph)'].quantile(0.90) #13.8\n",
    "print(dd['Wind_Speed(mph)'].skew())  #38.513"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling the Visibility column missing values with its mean\n",
    "dd_Windspeed=dd[\"Wind_Speed(mph)\"]\n",
    "#plotting the Pressure histogram\n",
    "dd_Windspeed.hist(bins=bin_size)\n",
    "#approximately follows normal distribution\n",
    "print(dd[\"Wind_Speed(mph)\"].isnull().sum())\n",
    "#filling the missing values with the concept of flooring and capping\n",
    "median_Windspeed=dd_Windspeed.median()\n",
    "mean_Windspeed=dd_Windspeed.mean()\n",
    "print(mean_Windspeed,median_Windspeed)\n",
    "#calculating the median and mean pressure and it is left-skewed ditribution\n",
    "dd[\"Wind_Speed(mph)\"] = np.where(dd[\"Wind_Speed(mph)\"] <quartile_10, quartile_10,dd['Wind_Speed(mph)'])\n",
    "dd[\"Wind_Speed(mph)\"] = np.where(dd[\"Wind_Speed(mph)\"] >quartile_90, quartile_90,dd['Wind_Speed(mph)'])\n",
    "print(dd['Visibility(mi)'].skew())\n",
    "#improved skew value from 38 to -1.84\n",
    "#new mean and median\n",
    "\n",
    "new_median_Windspeed=dd_Windspeed.median()\n",
    "new_mean_Windspeed=dd_Windspeed.mean()\n",
    "print(new_mean_Windspeed,new_median_Windspeed)\n",
    "dd[\"Wind_Speed(mph)\"].fillna(new_median_Windspeed, inplace=True)\n",
    "#ensuring that there are no more null values in visibility\n",
    "print(dd['Wind_Speed(mph)'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wind_Direction\n",
    "\n",
    "#filling the category with the mode \n",
    "mode_found=(dd['Wind_Direction'].mode())\n",
    "print(mode_found[0])\n",
    "dd[\"Wind_Direction\"].fillna(mode_found[0], inplace=True)\n",
    "print(dd['Wind_Direction'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dd.isnull().sum())\n",
    "\n",
    "\n",
    "\n",
    "#Weather_Condition\n",
    "#filling the category with the mode \n",
    "mode_found=(dd['Weather_Condition'].mode())\n",
    "print(mode_found[0])\n",
    "dd[\"Weather_Condition\"].fillna(mode_found[0], inplace=True)\n",
    "print(dd['Weather_Condition'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#City\n",
    "#filling the category with the mode \n",
    "mode_found=(dd['City'].mode())\n",
    "print(mode_found[0])\n",
    "dd[\"City\"].fillna(mode_found[0], inplace=True)\n",
    "print(dd['City'].isnull().sum())\n",
    "\n",
    "#Sunrise_sunset\n",
    "#filling the category with the mode \n",
    "mode_found=(dd['Sunrise_Sunset'].mode())\n",
    "print(mode_found[0])\n",
    "dd[\"Sunrise_Sunset\"].fillna(mode_found[0], inplace=True)\n",
    "print(dd['Sunrise_Sunset'].isnull().sum())\n",
    "#dropping the cols 'Zipcode','Timezone','Airport_Code' \n",
    "unwanted_cols_2=['Zipcode','Timezone','Airport_Code','Wind_Chill(F)','Precipitation(in)']\n",
    "dd.drop(unwanted_cols_2, axis=1,inplace=True)\n",
    "\n",
    "print(dd.isnull().mean())\n",
    "print(dd.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation plots for numerical features\n",
    "numerical_features=['Start_Lat','Start_Lng','Temperature(F)','Humidity(%)','Pressure(in)','Visibility(mi)','Wind_Speed(mph)']\n",
    "df_numerical=dd[numerical_features].copy()\n",
    "print(df_numerical.shape)\n",
    "sns.heatmap(df_numerical.corr(), annot = True)\n",
    "print(df_numerical.corr())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#removing the unwanted columns\n",
    "filtered_columns=['Start_Time','End_Time','Description','Street','State','Country','Unnamed: 0','ID']\n",
    "dd.drop(filtered_columns, axis=1,inplace=True)\n",
    "print(dd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing a standardisation on numerical columns such that mean is 0 and variance is 1\n",
    "# numerical features\n",
    "num_cols = ['Temperature(F)','Humidity(%)','Pressure(in)','Visibility(mi)','Wind_Speed(mph)','Distance(mi)']\n",
    "# apply standardization on numerical features\n",
    "for i in num_cols:\n",
    "    # fit on training data column\n",
    "    scale = StandardScaler().fit(dd[[i]])\n",
    "    # transform the training data column\n",
    "    dd[i] = scale.transform(dd[[i]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Approach using one hot encoding/pd.get_dummies for few columns\n",
    "#one hot encoding approach\n",
    "\n",
    "\n",
    "\n",
    "#Copying the dataframe into ds\n",
    "ds=dd.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using pd_getdummies i.e one hot encoding\n",
    "features_converted=['Side','Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout','Station','Stop','Traffic_Calming','Traffic_Signal','Sunrise_Sunset','Year','Month','Day','Hour','Weekday']\n",
    "for i in features_converted:\n",
    "    \n",
    "    ds = pd.concat([ds,pd.get_dummies(ds[i], prefix=i)],axis=1)\n",
    "    ds.drop([i],axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#using label encoding to the rest of the columns having object datatype\n",
    "features_label_encoding=['Source','City','County','Wind_Direction','Weather_Condition']\n",
    "labelencoder = LabelEncoder()\n",
    "# Assigning numerical values and storing in another column\n",
    "\n",
    "for i in features_label_encoding:\n",
    "    ds[i] = labelencoder.fit_transform(ds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target='Severity'\n",
    "# Create arrays for the features and the response variable\n",
    "# set X and y\n",
    "y = ds[target]\n",
    "X = ds.drop(target, axis=1)\n",
    "\n",
    "# Split the data set into training and testing data sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)\n",
    "lr = LogisticRegression(random_state=0)\n",
    "lr.fit(X_train,y_train)\n",
    "y_pred=lr.predict(X_test)\n",
    "\n",
    "# Get the accuracy score\n",
    "acc=accuracy_score(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "# put none to n_componenets to create explained variance vector \n",
    "# ( contain the percentage of variance explained by each of the principal components that we extracted here.)\n",
    "pca = PCA(n_components=2) \n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "expained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "#Fitting logistic Regression to the training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train_pca, y_train)\n",
    "\n",
    "#Prdicting the test set results\n",
    "y_pred = classifier.predict(X_test_pca)\n",
    "acc=accuracy_score(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Approach using labelencoding for all few columns\n",
    "#labelencoding approach\n",
    "\n",
    "\n",
    "\n",
    "#Copying the dataframe into dr\n",
    "\n",
    "dr=dd.copy()\n",
    "features_label_encoding=['Source','Side','City','County','Wind_Direction','Weather_Condition','Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout','Station','Stop','Traffic_Calming','Traffic_Signal','Sunrise_Sunset','Year','Month','Day','Hour','Weekday']\n",
    "labelencoder = LabelEncoder()\n",
    "# Assigning numerical values and storing in another column\n",
    "\n",
    "for i in features_label_encoding:\n",
    "    dr[i] = labelencoder.fit_transform(dr[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target='Severity'\n",
    "# Create arrays for the features and the response variable\n",
    "# set X and y\n",
    "y = dr[target]\n",
    "X = dr.drop(target, axis=1)\n",
    "# Split the data set into training and testing data sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)\n",
    "lr = LogisticRegression(random_state=0)\n",
    "lr.fit(X_train,y_train)\n",
    "y_pred=lr.predict(X_test)\n",
    "# Get the accuracy score\n",
    "acc=accuracy_score(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "# put none to n_componenets to create explained variance vector \n",
    "# ( contain the percentage of variance explained by each of the principal components that we extracted here.)\n",
    "pca = PCA(n_components=2) \n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "expained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "#Fitting logistic Regression to the training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train_pca, y_train)\n",
    "\n",
    "#Prdicting the test set results\n",
    "y_pred = classifier.predict(X_test_pca)\n",
    "acc=accuracy_score(y_test, y_pred)\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
